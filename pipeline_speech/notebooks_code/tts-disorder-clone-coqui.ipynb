{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d0a357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "%cd ../clone-voice-coqui/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390fb011-8d1f-4b55-9506-18d2faf394e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/is/lathifgalih-k/miniconda3/envs/yourtts/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0aaf8a",
   "metadata": {},
   "source": [
    "## ALS Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d602ae2",
   "metadata": {},
   "source": [
    "### XTTS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 3.9247093200683594\n",
      " > Real-time factor: 0.9970027708238172\n",
      "✅ Audio untuk A011/input_A011_02_BBP_NORMAL_segment_0.wav berhasil disimpan di 'output_audio/ALS/XTTSv2/A011_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.1480772495269775\n",
      " > Real-time factor: 0.2951303785682458\n",
      "✅ Audio untuk A002/input_A002_02_BBP_NORMAL_segment_0.wav berhasil disimpan di 'output_audio/ALS/XTTSv2/A002_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.3277246952056885\n",
      " > Real-time factor: 0.29243576723355275\n",
      "✅ Audio untuk A008/input_A008_02_BBP_NORMAL_segment_0.wav berhasil disimpan di 'output_audio/ALS/XTTSv2/A008_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.0999376773834229\n",
      " > Real-time factor: 0.29508499350672174\n",
      "✅ Audio untuk A016/input_A016_02_BBP_Normal_segment_0.wav berhasil disimpan di 'output_audio/ALS/XTTSv2/A016_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.4507482051849365\n",
      " > Real-time factor: 0.2932833167479083\n",
      "✅ Audio untuk A006/input_A006_02_BBP_NORMAL_al_segment_0.wav berhasil disimpan di 'output_audio/ALS/XTTSv2/A006_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.2113909721374512\n",
      " > Real-time factor: 0.2930398777386212\n",
      "✅ Audio untuk A015/input_A015_02_DDK_PATAKA_segment_0.wav berhasil disimpan di 'output_audio/ALS/XTTSv2/A015_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.505932331085205\n",
      " > Real-time factor: 0.2927582160779797\n",
      "✅ Audio untuk A017/input_A017_02_BBP_Normal_segment_0.wav berhasil disimpan di 'output_audio/ALS/XTTSv2/A017_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.7935402393341064\n",
      " > Real-time factor: 0.2987698105079555\n",
      "✅ Audio untuk A010/input_A010_02_BBP_NORMAL_segment_0.wav berhasil disimpan di 'output_audio/ALS/XTTSv2/A010_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.156226396560669\n",
      " > Real-time factor: 0.29029413422484457\n",
      "✅ Audio untuk A014/input_A014_02_BBP_Normal_segment_0.wav berhasil disimpan di 'output_audio/ALS/XTTSv2/A014_cloned.wav'\n"
     ]
    }
   ],
   "source": [
    "# Generate for audio in input audio\n",
    "import os\n",
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Inisialisasi TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")   \n",
    "\n",
    "# Path ke direktori input audio\n",
    "input_audio_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/dataset-toronto/ALS/Input_Audios/\"\n",
    "output_dir = \"output_audio/ALS/XTTSv2/\"\n",
    "\n",
    "# Loop melalui setiap file audio di direktori input\n",
    "for speaker in os.listdir(input_audio_dir):\n",
    "    speaker_path = os.path.join(input_audio_dir, speaker)\n",
    "    if os.path.isdir(speaker_path):\n",
    "        i = 0\n",
    "        for audio_file in os.listdir(speaker_path):\n",
    "            if i >= 1:\n",
    "                continue\n",
    "            \n",
    "            # Kondisi berbeda untuk A015 dan speaker lainnya\n",
    "            if speaker == \"A015\":\n",
    "                # Untuk A015, gunakan file dengan \"pataka\" dan \"segment_0\"\n",
    "                if \"pataka\" in audio_file.lower() and \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                    i += 1  # Hanya proses 1 file per speaker\n",
    "                    input_audio_path = os.path.join(speaker_path, audio_file)\n",
    "                    output_file = os.path.join(output_dir, f\"{speaker}_cloned.wav\")\n",
    "\n",
    "                    # Jalankan TTS untuk setiap file audio\n",
    "                    tts.tts_to_file(\n",
    "                        text=\"Even for a British Open, this is different\",\n",
    "                        speaker_wav=input_audio_path,\n",
    "                        language=\"en\",\n",
    "                        file_path=output_file\n",
    "                    )\n",
    "                    print(f\"✅ Audio untuk {speaker}/{audio_file} berhasil disimpan di '{output_file}'\")\n",
    "            else:\n",
    "                # Untuk speaker selain A015, gunakan file dengan \"normal\" dan \"segment_0\"\n",
    "                if \"normal\" in audio_file.lower() and \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                    i += 1  # Hanya proses 1 file per speaker\n",
    "                    input_audio_path = os.path.join(speaker_path, audio_file)\n",
    "                    output_file = os.path.join(output_dir, f\"{speaker}_cloned.wav\")\n",
    "\n",
    "                    # Jalankan TTS untuk setiap file audio\n",
    "                    tts.tts_to_file(\n",
    "                        text=\"Even for a British Open, this is different\",\n",
    "                        speaker_wav=input_audio_path,\n",
    "                        language=\"en\",\n",
    "                        file_path=output_file\n",
    "                    )\n",
    "                    print(f\"✅ Audio untuk {speaker}/{audio_file} berhasil disimpan di '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb50cd",
   "metadata": {},
   "source": [
    "### YourTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162ddc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 11.317098379135132\n",
      " > Real-time factor: 2.7840340416076583\n",
      "✅ Audio untuk A011/input_A011_02_BBP_NORMAL_segment_0.wav berhasil disimpan di 'output_audio/ALS/YourTTS/A011_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.47563767433166504\n",
      " > Real-time factor: 0.1531844361776699\n",
      "✅ Audio untuk A002/input_A002_02_BBP_NORMAL_segment_0.wav berhasil disimpan di 'output_audio/ALS/YourTTS/A002_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.6039924621582031\n",
      " > Real-time factor: 0.1905940240322509\n",
      "✅ Audio untuk A008/input_A008_02_BBP_NORMAL_segment_0.wav berhasil disimpan di 'output_audio/ALS/YourTTS/A008_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.36189961433410645\n",
      " > Real-time factor: 0.0825312689473447\n",
      "✅ Audio untuk A016/input_A016_02_BBP_Normal_segment_0.wav berhasil disimpan di 'output_audio/ALS/YourTTS/A016_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.31761670112609863\n",
      " > Real-time factor: 0.09103373491719652\n",
      "✅ Audio untuk A006/input_A006_02_BBP_NORMAL_al_segment_0.wav berhasil disimpan di 'output_audio/ALS/YourTTS/A006_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.3127439022064209\n",
      " > Real-time factor: 0.06785504495691493\n",
      "✅ Audio untuk A015/input_A015_02_DDK_PATAKA_segment_0.wav berhasil disimpan di 'output_audio/ALS/YourTTS/A015_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.3457205295562744\n",
      " > Real-time factor: 0.08372984489132343\n",
      "✅ Audio untuk A017/input_A017_02_BBP_Normal_segment_0.wav berhasil disimpan di 'output_audio/ALS/YourTTS/A017_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.37493348121643066\n",
      " > Real-time factor: 0.10896061645348175\n",
      "✅ Audio untuk A010/input_A010_02_BBP_NORMAL_segment_0.wav berhasil disimpan di 'output_audio/ALS/YourTTS/A010_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.32665443420410156\n",
      " > Real-time factor: 0.09955941304605351\n",
      "✅ Audio untuk A014/input_A014_02_BBP_Normal_segment_0.wav berhasil disimpan di 'output_audio/ALS/YourTTS/A014_cloned.wav'\n"
     ]
    }
   ],
   "source": [
    "# Generate for audio in input audio\n",
    "import os\n",
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Inisialisasi TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/your_tts\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")   \n",
    "\n",
    "# Path ke direktori input audio\n",
    "input_audio_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/dataset-toronto/ALS/Input_Audios/\"\n",
    "output_dir = \"output_audio/ALS/YourTTS/\"\n",
    "\n",
    "# Loop melalui setiap file audio di direktori input\n",
    "for speaker in os.listdir(input_audio_dir):\n",
    "    speaker_path = os.path.join(input_audio_dir, speaker)\n",
    "    if os.path.isdir(speaker_path):\n",
    "        i = 0\n",
    "        for audio_file in os.listdir(speaker_path):\n",
    "            if i >= 1:\n",
    "                continue\n",
    "            \n",
    "            # Kondisi berbeda untuk A015 dan speaker lainnya\n",
    "            if speaker == \"A015\":\n",
    "                # Untuk A015, gunakan file dengan \"pataka\" dan \"segment_0\"\n",
    "                if \"pataka\" in audio_file.lower() and \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                    i += 1  # Hanya proses 1 file per speaker\n",
    "                    input_audio_path = os.path.join(speaker_path, audio_file)\n",
    "                    output_file = os.path.join(output_dir, f\"{speaker}_cloned.wav\")\n",
    "\n",
    "                    # Jalankan TTS untuk setiap file audio\n",
    "                    tts.tts_to_file(\n",
    "                        text=\"Even for a British Open, this is different\",\n",
    "                        speaker_wav=input_audio_path,\n",
    "                        language=\"en\",\n",
    "                        file_path=output_file\n",
    "                    )\n",
    "                    print(f\"✅ Audio untuk {speaker}/{audio_file} berhasil disimpan di '{output_file}'\")\n",
    "            else:\n",
    "                # Untuk speaker selain A015, gunakan file dengan \"normal\" dan \"segment_0\"\n",
    "                if \"normal\" in audio_file.lower() and \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                    i += 1  # Hanya proses 1 file per speaker\n",
    "                    input_audio_path = os.path.join(speaker_path, audio_file)\n",
    "                    output_file = os.path.join(output_dir, f\"{speaker}_cloned.wav\")\n",
    "\n",
    "                    # Jalankan TTS untuk setiap file audio\n",
    "                    tts.tts_to_file(\n",
    "                        text=\"Even for a British Open, this is different\",\n",
    "                        speaker_wav=input_audio_path,\n",
    "                        language=\"en\",\n",
    "                        file_path=output_file\n",
    "                    )\n",
    "                    print(f\"✅ Audio untuk {speaker}/{audio_file} berhasil disimpan di '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fdd37",
   "metadata": {},
   "source": [
    "## Stroke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8dff57",
   "metadata": {},
   "source": [
    "### XTTSv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb734cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.1831767559051514\n",
      " > Real-time factor: 0.3174158977480605\n",
      "✅ Audio untuk S009/input_S009_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/XTTSv2/S009_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.4324138164520264\n",
      " > Real-time factor: 0.2986904661519063\n",
      "✅ Audio untuk S003/input_S003_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/XTTSv2/S003_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.058121681213379\n",
      " > Real-time factor: 0.3037333767802933\n",
      "✅ Audio untuk S013/input_S013_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/XTTSv2/S013_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.4524872303009033\n",
      " > Real-time factor: 0.2999713718355211\n",
      "✅ Audio untuk S007/input_S007_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/XTTSv2/S007_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.4277057647705078\n",
      " > Real-time factor: 0.2948534402928752\n",
      "✅ Audio untuk S005/input_S005_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/XTTSv2/S005_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 2.1265182495117188\n",
      " > Real-time factor: 0.3032186200319025\n",
      "✅ Audio untuk S002/input_S002_02_DDK_PA_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/XTTSv2/S002_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.983846664428711\n",
      " > Real-time factor: 0.2955983008342326\n",
      "✅ Audio untuk S008/input_S008_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/XTTSv2/S008_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.1900904178619385\n",
      " > Real-time factor: 0.2878871962639958\n",
      "✅ Audio untuk S011/input_S011_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/XTTSv2/S011_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.3849389553070068\n",
      " > Real-time factor: 0.28879089087342547\n",
      "✅ Audio untuk S006/input_S006_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/XTTSv2/S006_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.9766602516174316\n",
      " > Real-time factor: 0.3069463875165959\n",
      "✅ Audio untuk S001/input_S001_02_DDK_PA_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/XTTSv2/S001_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 1.463301658630371\n",
      " > Real-time factor: 0.3051312752761356\n",
      "✅ Audio untuk S012/input_S012_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/XTTSv2/S012_cloned.wav'\n"
     ]
    }
   ],
   "source": [
    "# Experiment Generate for audio in input audio\n",
    "import os\n",
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Inisialisasi TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")   \n",
    "\n",
    "# Path ke direktori input audio\n",
    "input_audio_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/vits/dataset-toronto/Stroke/Input_Audios/\"\n",
    "output_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/XTTSv2/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for speaker in os.listdir(input_audio_dir):\n",
    "    speaker_path = os.path.join(input_audio_dir, speaker)\n",
    "    if os.path.isdir(speaker_path):\n",
    "        i = 0\n",
    "        for audio_file in os.listdir(speaker_path):\n",
    "            if i >= 1:\n",
    "                continue\n",
    "            \n",
    "            # Untuk speaker selain A015, gunakan file dengan \"normal\" dan \"segment_0\"\n",
    "            if \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                i += 1  # Hanya proses 1 file per speaker\n",
    "                input_audio_path = os.path.join(speaker_path, audio_file)\n",
    "                output_file = os.path.join(output_dir, f\"{speaker}_cloned.wav\")\n",
    "\n",
    "                # Jalankan TTS untuk setiap file audio\n",
    "                tts.tts_to_file(\n",
    "                    text=\"Even for a British Open, this is different\",\n",
    "                    speaker_wav=input_audio_path,\n",
    "                    language=\"en\",\n",
    "                    file_path=output_file\n",
    "                )\n",
    "                print(f\"✅ Audio untuk {speaker}/{audio_file} berhasil disimpan di '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aac18c",
   "metadata": {},
   "source": [
    "### YourTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bbdc11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/your_tts is already downloaded.\n",
      " > Using model: vits\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model fully restored. \n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:64\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.97\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:False\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:True\n",
      " | > db_level:-27.0\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:160\n",
      " | > win_length:400\n",
      " > External Speaker Encoder Loaded !!\n",
      " > initialization of language-embedding layers.\n",
      " > Model fully restored. \n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:64\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.97\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:False\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:True\n",
      " | > db_level:-27.0\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:160\n",
      " | > win_length:400\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.25410008430480957\n",
      " > Real-time factor: 0.0664313945894927\n",
      "✅ Audio untuk S009/input_S009_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/YourTTS/S009_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.23085403442382812\n",
      " > Real-time factor: 0.07359070271719098\n",
      "✅ Audio untuk S003/input_S003_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/YourTTS/S003_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.21791744232177734\n",
      " > Real-time factor: 0.06303657573670157\n",
      "✅ Audio untuk S013/input_S013_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/YourTTS/S013_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.2276134490966797\n",
      " > Real-time factor: 0.052288869537486724\n",
      "✅ Audio untuk S007/input_S007_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/YourTTS/S007_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.22475147247314453\n",
      " > Real-time factor: 0.05975843458472335\n",
      "✅ Audio untuk S005/input_S005_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/YourTTS/S005_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.24959039688110352\n",
      " > Real-time factor: 0.07797263257766433\n",
      "✅ Audio untuk S002/input_S002_02_DDK_PA_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/YourTTS/S002_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.2176203727722168\n",
      " > Real-time factor: 0.061806410898101904\n",
      "✅ Audio untuk S008/input_S008_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/YourTTS/S008_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.24036502838134766\n",
      " > Real-time factor: 0.07050895523066814\n",
      "✅ Audio untuk S011/input_S011_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/YourTTS/S011_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.23267412185668945\n",
      " > Real-time factor: 0.06239584925092235\n",
      "✅ Audio untuk S006/input_S006_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/YourTTS/S006_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.22577643394470215\n",
      " > Real-time factor: 0.06915051575641719\n",
      "✅ Audio untuk S001/input_S001_02_DDK_PA_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/YourTTS/S001_cloned.wav'\n",
      " > Text splitted to sentences.\n",
      "['Even for a British Open, this is different']\n",
      " > Processing time: 0.22474169731140137\n",
      " > Real-time factor: 0.06213483475570953\n",
      "✅ Audio untuk S012/input_S012_02_BBP_NORMAL_segment_0.wav berhasil disimpan di '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/YourTTS/S012_cloned.wav'\n"
     ]
    }
   ],
   "source": [
    "# Experiment Generate for audio in input audio\n",
    "import os\n",
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Inisialisasi TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/your_tts\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")   \n",
    "\n",
    "# Path ke direktori input audio\n",
    "input_audio_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/vits/dataset-toronto/Stroke/Input_Audios/\"\n",
    "output_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/Stroke/YourTTS/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for speaker in os.listdir(input_audio_dir):\n",
    "    speaker_path = os.path.join(input_audio_dir, speaker)\n",
    "    if os.path.isdir(speaker_path):\n",
    "        i = 0\n",
    "        for audio_file in os.listdir(speaker_path):\n",
    "            if i >= 1:\n",
    "                continue\n",
    "            \n",
    "            # Untuk speaker selain A015, gunakan file dengan \"normal\" dan \"segment_0\"\n",
    "            if \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                i += 1  # Hanya proses 1 file per speaker\n",
    "                input_audio_path = os.path.join(speaker_path, audio_file)\n",
    "                output_file = os.path.join(output_dir, f\"{speaker}_cloned.wav\")\n",
    "\n",
    "                # Jalankan TTS untuk setiap file audio\n",
    "                tts.tts_to_file(\n",
    "                    text=\"Even for a British Open, this is different\",\n",
    "                    speaker_wav=input_audio_path,\n",
    "                    language=\"en\",\n",
    "                    file_path=output_file\n",
    "                )\n",
    "                print(f\"✅ Audio untuk {speaker}/{audio_file} berhasil disimpan di '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def5725",
   "metadata": {},
   "source": [
    "### Evaluation By Cosine Similarity of Speaker Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93177000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cuda in 0.03 seconds.\n",
      "Cosine Similarity: 0.5996\n"
     ]
    }
   ],
   "source": [
    "# ALS\n",
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load audio files (pastikan formatnya .wav dan durasi minimal ~1 detik)\n",
    "input_audio = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/dataset-toronto/ALS/Input_Audios/A002/input_A002_02_BBP_NORMAL_segment_0.wav\"\n",
    "original_audio = preprocess_wav(Path(input_audio))\n",
    "cloned_audio = preprocess_wav(Path(\"/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/exp/ALS/YourTTS/A002_BBP_cloned.wav\"))\n",
    "\n",
    "# Init encoder\n",
    "encoder = VoiceEncoder()\n",
    "\n",
    "# Extract embeddings (1 vector per utterance)\n",
    "original_embed = encoder.embed_utterance(original_audio)\n",
    "cloned_embed = encoder.embed_utterance(cloned_audio)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cos_sim = 1 - cosine(original_embed, cloned_embed)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc36920d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cuda in 0.02 seconds.\n",
      "Cosine Similarity: 0.6866\n"
     ]
    }
   ],
   "source": [
    "# Stroke\n",
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load audio files (pastikan formatnya .wav dan durasi minimal ~1 detik)\n",
    "input_audio = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/vits/dataset-toronto/Stroke/Input_Audios/S003/input_S003_02_BBP_NORMAL_segment_1.wav\"\n",
    "original_audio = preprocess_wav(Path(input_audio))\n",
    "cloned_audio = preprocess_wav(Path(\"/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/exp/Stroke/YourTTS/S003_BBP_cloned.wav\"))\n",
    "\n",
    "# Init encoder\n",
    "encoder = VoiceEncoder()\n",
    "\n",
    "# Extract embeddings (1 vector per utterance)\n",
    "original_embed = encoder.embed_utterance(original_audio)\n",
    "cloned_embed = encoder.embed_utterance(cloned_audio)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cos_sim = 1 - cosine(original_embed, cloned_embed)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea66b8",
   "metadata": {},
   "source": [
    "### TTS for ALS n Stroke for xtts n yourtts - Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0fa8c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Initializing XTTSv2...\n",
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n",
      "\n",
      "📂 Processing ALS dataset with XTTSv2...\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 1.6723732948303223\n",
      " > Real-time factor: 0.28805642381427793\n",
      "✅ A011/input_A011_02_BBP_NORMAL_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/XTTSv2/A011_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 1.7324891090393066\n",
      " > Real-time factor: 0.29604297004275193\n",
      "✅ A002/input_A002_02_BBP_NORMAL_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a pu...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/XTTSv2/A002_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 1.9656422138214111\n",
      " > Real-time factor: 0.2903741747156858\n",
      "✅ A008/input_A008_02_BBP_NORMAL_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a pu...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/XTTSv2/A008_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 1.9598667621612549\n",
      " > Real-time factor: 0.28952099706329504\n",
      "✅ A016/input_A016_02_BBP_Normal_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/XTTSv2/A016_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 1.5336525440216064\n",
      " > Real-time factor: 0.2896484736507848\n",
      "✅ A006/input_A006_02_BBP_NORMAL_al_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/XTTSv2/A006_cloned.wav'\n",
      "ini teks nya Pata ka pata\n",
      " > Text splitted to sentences.\n",
      "['Pata ka pata']\n",
      " > Processing time: 1.4785988330841064\n",
      " > Real-time factor: 0.29006320524470236\n",
      "✅ A015/input_A015_02_DDK_PATAKA_segment_0.wav -> 'Pata ka pata...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/XTTSv2/A015_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 2.056779623031616\n",
      " > Real-time factor: 0.29327464231665246\n",
      "✅ A017/input_A017_02_BBP_Normal_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/XTTSv2/A017_cloned.wav'\n",
      "ini teks nya buy bobby a puppy buy bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['buy bobby a puppy buy bobby a puppy']\n",
      " > Processing time: 2.174126625061035\n",
      " > Real-time factor: 0.29302868021146594\n",
      "✅ A010/input_A010_02_BBP_NORMAL_segment_0.wav -> 'buy bobby a puppy buy bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/XTTSv2/A010_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 1.7102856636047363\n",
      " > Real-time factor: 0.2871223571878764\n",
      "✅ A014/input_A014_02_BBP_Normal_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/XTTSv2/A014_cloned.wav'\n",
      "\n",
      "📂 Processing Stroke dataset with XTTSv2...\n",
      "ini teks nya By Bobby a puppy By Bobby a puppy By Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['By Bobby a puppy By Bobby a puppy By Bobby a puppy']\n",
      " > Processing time: 1.6316709518432617\n",
      " > Real-time factor: 0.2885469691401251\n",
      "✅ S009/input_S009_02_BBP_NORMAL_segment_0.wav -> 'By Bobby a puppy By Bobby a puppy By Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/XTTSv2/S009_cloned.wav'\n",
      "ini teks nya Bobby a puppy By Bobby a puppy By Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Bobby a puppy By Bobby a puppy By Bobby a puppy']\n",
      " > Processing time: 2.3289670944213867\n",
      " > Real-time factor: 0.2954079868384237\n",
      "✅ S003/input_S003_02_BBP_NORMAL_segment_0.wav -> 'Bobby a puppy By Bobby a puppy By Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/XTTSv2/S003_cloned.wav'\n",
      "ini teks nya By bobby a puppy by bobby a puppy by\n",
      " > Text splitted to sentences.\n",
      "['By bobby a puppy by bobby a puppy by']\n",
      " > Processing time: 2.277078151702881\n",
      " > Real-time factor: 0.29270574832716467\n",
      "✅ S013/input_S013_02_BBP_NORMAL_segment_0.wav -> 'By bobby a puppy by bobby a puppy by...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/XTTSv2/S013_cloned.wav'\n",
      "ini teks nya Bobby a puppy By Bobby a puppy By Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Bobby a puppy By Bobby a puppy By Bobby a puppy']\n",
      " > Processing time: 1.7431294918060303\n",
      " > Real-time factor: 0.2875913241823519\n",
      "✅ S007/input_S007_02_BBP_NORMAL_segment_0.wav -> 'Bobby a puppy By Bobby a puppy By Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/XTTSv2/S007_cloned.wav'\n",
      "ini teks nya By Bobby a puppy By Bobby a puppy By Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['By Bobby a puppy By Bobby a puppy By Bobby a puppy']\n",
      " > Processing time: 1.661562204360962\n",
      " > Real-time factor: 0.2914395332677804\n",
      "✅ S005/input_S005_02_BBP_NORMAL_segment_0.wav -> 'By Bobby a puppy By Bobby a puppy By Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/XTTSv2/S005_cloned.wav'\n",
      "ini teks nya Pa pa pa pa pa pa pa pa pa pa pa\n",
      " > Text splitted to sentences.\n",
      "['Pa pa pa pa pa pa pa pa pa pa pa']\n",
      " > Processing time: 2.0876758098602295\n",
      " > Real-time factor: 0.2933100443942939\n",
      "✅ S002/input_S002_02_DDK_PA_segment_0.wav -> 'Pa pa pa pa pa pa pa pa pa pa pa...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/XTTSv2/S002_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 1.692770004272461\n",
      " > Real-time factor: 0.2915696365626778\n",
      "✅ S008/input_S008_02_BBP_NORMAL_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a pu...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/XTTSv2/S008_cloned.wav'\n",
      "ini teks nya By Bobby a puppy By bobby a puppy by bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['By Bobby a puppy By bobby a puppy by bobby a puppy']\n",
      " > Processing time: 2.0601117610931396\n",
      " > Real-time factor: 0.29133827816895674\n",
      "✅ S011/input_S011_02_BBP_NORMAL_segment_0.wav -> 'By Bobby a puppy By bobby a puppy by bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/XTTSv2/S011_cloned.wav'\n",
      "ini teks nya Bobby a puppy By Bobby a puppy By Bobby\n",
      " > Text splitted to sentences.\n",
      "['Bobby a puppy By Bobby a puppy By Bobby']\n",
      " > Processing time: 1.5803263187408447\n",
      " > Real-time factor: 0.28472835769574145\n",
      "✅ S006/input_S006_02_BBP_NORMAL_segment_0.wav -> 'Bobby a puppy By Bobby a puppy By Bobby...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/XTTSv2/S006_cloned.wav'\n",
      "ini teks nya Pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa\n",
      " > Text splitted to sentences.\n",
      "['Pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa']\n",
      " > Processing time: 2.690095901489258\n",
      " > Real-time factor: 0.2974158374841463\n",
      "✅ S001/input_S001_02_DDK_PA_segment_0.wav -> 'Pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/XTTSv2/S001_cloned.wav'\n",
      "ini teks nya Bobby a puppy by bobby a puppy by bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Bobby a puppy by bobby a puppy by bobby a puppy']\n",
      " > Processing time: 1.831087589263916\n",
      " > Real-time factor: 0.28988714347551225\n",
      "✅ S012/input_S012_02_BBP_NORMAL_segment_0.wav -> 'Bobby a puppy by bobby a puppy by bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/XTTSv2/S012_cloned.wav'\n",
      "\n",
      "🚀 Initializing YourTTS...\n",
      " > tts_models/multilingual/multi-dataset/your_tts is already downloaded.\n",
      " > Using model: vits\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model fully restored. \n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:64\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.97\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:False\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:True\n",
      " | > db_level:-27.0\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:160\n",
      " | > win_length:400\n",
      " > External Speaker Encoder Loaded !!\n",
      " > initialization of language-embedding layers.\n",
      " > Model fully restored. \n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:64\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.97\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:False\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:True\n",
      " | > db_level:-27.0\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:160\n",
      " | > win_length:400\n",
      "\n",
      "📂 Processing ALS dataset with YourTTS...\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 0.23934292793273926\n",
      " > Real-time factor: 0.07087442343285143\n",
      "✅ A011/input_A011_02_BBP_NORMAL_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/YourTTS/A011_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 0.179962158203125\n",
      " > Real-time factor: 0.03987639224531908\n",
      "✅ A002/input_A002_02_BBP_NORMAL_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a pu...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/YourTTS/A002_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 0.21306538581848145\n",
      " > Real-time factor: 0.04622811582088988\n",
      "✅ A008/input_A008_02_BBP_NORMAL_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a pu...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/YourTTS/A008_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 0.21930885314941406\n",
      " > Real-time factor: 0.05831131431784474\n",
      "✅ A016/input_A016_02_BBP_Normal_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/YourTTS/A016_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 0.21827030181884766\n",
      " > Real-time factor: 0.06061380222683912\n",
      "✅ A006/input_A006_02_BBP_NORMAL_al_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/YourTTS/A006_cloned.wav'\n",
      "ini teks nya Pata ka pata\n",
      " > Text splitted to sentences.\n",
      "['Pata ka pata']\n",
      " > Processing time: 0.23589348793029785\n",
      " > Real-time factor: 0.12702934191184592\n",
      "✅ A015/input_A015_02_DDK_PATAKA_segment_0.wav -> 'Pata ka pata...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/YourTTS/A015_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 0.22140145301818848\n",
      " > Real-time factor: 0.05716536354716976\n",
      "✅ A017/input_A017_02_BBP_Normal_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/YourTTS/A017_cloned.wav'\n",
      "ini teks nya buy bobby a puppy buy bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['buy bobby a puppy buy bobby a puppy']\n",
      " > Processing time: 0.2090756893157959\n",
      " > Real-time factor: 0.06403543317482263\n",
      "✅ A010/input_A010_02_BBP_NORMAL_segment_0.wav -> 'buy bobby a puppy buy bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/YourTTS/A010_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 0.21034717559814453\n",
      " > Real-time factor: 0.0759650327187232\n",
      "✅ A014/input_A014_02_BBP_Normal_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/YourTTS/A014_cloned.wav'\n",
      "\n",
      "📂 Processing Stroke dataset with YourTTS...\n",
      "ini teks nya By Bobby a puppy By Bobby a puppy By Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['By Bobby a puppy By Bobby a puppy By Bobby a puppy']\n",
      " > Processing time: 0.2123415470123291\n",
      " > Real-time factor: 0.049324401164304095\n",
      "✅ S009/input_S009_02_BBP_NORMAL_segment_0.wav -> 'By Bobby a puppy By Bobby a puppy By Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/YourTTS/S009_cloned.wav'\n",
      "ini teks nya Bobby a puppy By Bobby a puppy By Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Bobby a puppy By Bobby a puppy By Bobby a puppy']\n",
      " > Processing time: 0.17821407318115234\n",
      " > Real-time factor: 0.04738475755946619\n",
      "✅ S003/input_S003_02_BBP_NORMAL_segment_0.wav -> 'Bobby a puppy By Bobby a puppy By Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/YourTTS/S003_cloned.wav'\n",
      "ini teks nya By bobby a puppy by bobby a puppy by\n",
      " > Text splitted to sentences.\n",
      "['By bobby a puppy by bobby a puppy by']\n",
      " > Processing time: 0.1774899959564209\n",
      " > Real-time factor: 0.05436140764362049\n",
      "✅ S013/input_S013_02_BBP_NORMAL_segment_0.wav -> 'By bobby a puppy by bobby a puppy by...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/YourTTS/S013_cloned.wav'\n",
      "ini teks nya Bobby a puppy By Bobby a puppy By Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Bobby a puppy By Bobby a puppy By Bobby a puppy']\n",
      " > Processing time: 0.21244406700134277\n",
      " > Real-time factor: 0.04641557067977775\n",
      "✅ S007/input_S007_02_BBP_NORMAL_segment_0.wav -> 'Bobby a puppy By Bobby a puppy By Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/YourTTS/S007_cloned.wav'\n",
      "ini teks nya By Bobby a puppy By Bobby a puppy By Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['By Bobby a puppy By Bobby a puppy By Bobby a puppy']\n",
      " > Processing time: 0.21802544593811035\n",
      " > Real-time factor: 0.04091301293640652\n",
      "✅ S005/input_S005_02_BBP_NORMAL_segment_0.wav -> 'By Bobby a puppy By Bobby a puppy By Bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/YourTTS/S005_cloned.wav'\n",
      "ini teks nya Pa pa pa pa pa pa pa pa pa pa pa\n",
      " > Text splitted to sentences.\n",
      "['Pa pa pa pa pa pa pa pa pa pa pa']\n",
      " > Processing time: 0.18394088745117188\n",
      " > Real-time factor: 0.06642863396575366\n",
      "✅ S002/input_S002_02_DDK_PA_segment_0.wav -> 'Pa pa pa pa pa pa pa pa pa pa pa...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/YourTTS/S002_cloned.wav'\n",
      "ini teks nya Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a puppy']\n",
      " > Processing time: 0.19203543663024902\n",
      " > Real-time factor: 0.04255161458680457\n",
      "✅ S008/input_S008_02_BBP_NORMAL_segment_0.wav -> 'Buy Bobby a puppy Buy Bobby a puppy Buy Bobby a pu...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/YourTTS/S008_cloned.wav'\n",
      "ini teks nya By Bobby a puppy By bobby a puppy by bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['By Bobby a puppy By bobby a puppy by bobby a puppy']\n",
      " > Processing time: 0.18297123908996582\n",
      " > Real-time factor: 0.04554922556384511\n",
      "✅ S011/input_S011_02_BBP_NORMAL_segment_0.wav -> 'By Bobby a puppy By bobby a puppy by bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/YourTTS/S011_cloned.wav'\n",
      "ini teks nya Bobby a puppy By Bobby a puppy By Bobby\n",
      " > Text splitted to sentences.\n",
      "['Bobby a puppy By Bobby a puppy By Bobby']\n",
      " > Processing time: 0.2109529972076416\n",
      " > Real-time factor: 0.05937320495571112\n",
      "✅ S006/input_S006_02_BBP_NORMAL_segment_0.wav -> 'Bobby a puppy By Bobby a puppy By Bobby...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/YourTTS/S006_cloned.wav'\n",
      "ini teks nya Pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa\n",
      " > Text splitted to sentences.\n",
      "['Pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa']\n",
      " > Processing time: 0.18287229537963867\n",
      " > Real-time factor: 0.047413091879605566\n",
      "✅ S001/input_S001_02_DDK_PA_segment_0.wav -> 'Pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa pa...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/YourTTS/S001_cloned.wav'\n",
      "ini teks nya Bobby a puppy by bobby a puppy by bobby a puppy\n",
      " > Text splitted to sentences.\n",
      "['Bobby a puppy by bobby a puppy by bobby a puppy']\n",
      " > Processing time: 0.21130967140197754\n",
      " > Real-time factor: 0.04537463418552234\n",
      "✅ S012/input_S012_02_BBP_NORMAL_segment_0.wav -> 'Bobby a puppy by bobby a puppy by bobby a puppy...' saved to '/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/YourTTS/S012_cloned.wav'\n"
     ]
    }
   ],
   "source": [
    "# TTS with corresponding text from annotations (Updated for both ALS and Stroke)\n",
    "import os\n",
    "import torch\n",
    "from TTS.api import TTS\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def load_annotations(annotation_file):\n",
    "    \"\"\"Load annotations and create a mapping from audio file to text\"\"\"\n",
    "    annotations = {}\n",
    "    with open(annotation_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split('|')\n",
    "                if len(parts) >= 3:\n",
    "                    audio_path = parts[0]\n",
    "                    text = parts[2].strip()\n",
    "                    # Extract filename from path\n",
    "                    filename = os.path.basename(audio_path)\n",
    "                    annotations[filename] = text\n",
    "    return annotations\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove punctuation from text\"\"\"\n",
    "    # Hanya simpan huruf, angka, dan spasi\n",
    "    return re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "def generate_tts_with_annotations():\n",
    "    \"\"\"Generate TTS using corresponding text from annotations\"\"\"\n",
    "    \n",
    "    # Load ALS annotations\n",
    "    als_annotation_file = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/vits/filelists/als_speaker_annotations.txt\"\n",
    "    als_annotations = load_annotations(als_annotation_file)\n",
    "    \n",
    "    # Load Stroke annotations\n",
    "    stroke_annotation_file = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/dataset-toronto/stroke_speaker_annotations.txt\"\n",
    "    stroke_annotations = load_annotations(stroke_annotation_file)\n",
    "    \n",
    "    # Models to use\n",
    "    models = {\n",
    "        \"XTTSv2\": \"tts_models/multilingual/multi-dataset/xtts_v2\",\n",
    "        \"YourTTS\": \"tts_models/multilingual/multi-dataset/your_tts\"\n",
    "    }\n",
    "    \n",
    "    # Datasets configuration\n",
    "    datasets = {\n",
    "        \"ALS\": {\n",
    "            \"input_dir\": \"/home/is/lathifgalih-k/research_naist/multimodal-clone/dataset-toronto/ALS/Input_Audios/\",\n",
    "            \"has_annotations\": True,\n",
    "            \"annotations\": als_annotations,\n",
    "            \"default_text\": \"Even for a British Open, this is different\"\n",
    "        },\n",
    "        \"Stroke\": {\n",
    "            \"input_dir\": \"/home/is/lathifgalih-k/research_naist/multimodal-clone/vits/dataset-toronto/Stroke/Input_Audios/\",\n",
    "            \"has_annotations\": True,\n",
    "            \"annotations\": stroke_annotations,\n",
    "            \"default_text\": \"Even for a British Open, this is different\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Base output directory\n",
    "    base_output_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/\"\n",
    "    \n",
    "    for model_name, model_path in models.items():\n",
    "        print(f\"\\n🚀 Initializing {model_name}...\")\n",
    "        tts = TTS(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        for dataset_name, dataset_config in datasets.items():\n",
    "            print(f\"\\n📂 Processing {dataset_name} dataset with {model_name}...\")\n",
    "            \n",
    "            input_audio_dir = dataset_config[\"input_dir\"]\n",
    "            output_dir = os.path.join(base_output_dir, dataset_name, model_name)\n",
    "            annotations = dataset_config[\"annotations\"]\n",
    "            \n",
    "            # Create output directory\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            \n",
    "            # Process each speaker\n",
    "            if os.path.exists(input_audio_dir):\n",
    "                for speaker in os.listdir(input_audio_dir):\n",
    "                    speaker_path = os.path.join(input_audio_dir, speaker)\n",
    "                    if os.path.isdir(speaker_path):\n",
    "                        i = 0\n",
    "                        for audio_file in os.listdir(speaker_path):\n",
    "                            if i >= 1:  # Only process 1 file per speaker\n",
    "                                continue\n",
    "                            \n",
    "                            # File selection logic based on dataset and speaker\n",
    "                            should_process = False\n",
    "                            if dataset_name == \"ALS\":\n",
    "                                if speaker == \"A015\":\n",
    "                                    # For A015, use pataka files\n",
    "                                    if \"pataka\" in audio_file.lower() and \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                                        should_process = True\n",
    "                                else:\n",
    "                                    # For other ALS speakers, use normal files\n",
    "                                    if \"normal\" in audio_file.lower() and \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                                        should_process = True\n",
    "                            else:  # Stroke\n",
    "                                # For Stroke, use segment_0 files\n",
    "                                if \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                                    should_process = True\n",
    "                            \n",
    "                            if should_process:\n",
    "                                i += 1\n",
    "                                input_audio_path = os.path.join(speaker_path, audio_file)\n",
    "                                \n",
    "                                # Determine text to synthesize\n",
    "                                text_to_synthesize = None\n",
    "                                \n",
    "                                # Look for matching annotation\n",
    "                                if annotations:\n",
    "                                    for annotation_filename, annotation_text in annotations.items():\n",
    "                                        if audio_file == annotation_filename:\n",
    "                                            text_to_synthesize = annotation_text\n",
    "                                            break\n",
    "                                \n",
    "                                # If not found, use default text\n",
    "                                if not text_to_synthesize:\n",
    "                                    text_to_synthesize = dataset_config[\"default_text\"]\n",
    "                                    print(f\"⚠️  No annotation found for {audio_file}, using default text\")\n",
    "                                \n",
    "                                # Generate output filename\n",
    "                                output_file = os.path.join(output_dir, f\"{speaker}_cloned.wav\")\n",
    "                                \n",
    "                                # Bersihkan teks dari tanda baca\n",
    "                                text_to_synthesize = clean_text(text_to_synthesize)\n",
    "                                \n",
    "                                try:\n",
    "                                    print(\"ini teks nya\", text_to_synthesize)\n",
    "                                    # Run TTS\n",
    "                                    tts.tts_to_file(\n",
    "                                        text=text_to_synthesize,\n",
    "                                        speaker_wav=input_audio_path,\n",
    "                                        language=\"en\",\n",
    "                                        file_path=output_file\n",
    "                                    )\n",
    "                                    print(f\"✅ {speaker}/{audio_file} -> '{text_to_synthesize[:50]}...' saved to '{output_file}'\")\n",
    "                                    \n",
    "                                except Exception as e:\n",
    "                                    print(f\"❌ Error processing {speaker}/{audio_file}: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"⚠️  Input directory not found: {input_audio_dir}\")\n",
    "\n",
    "# Run the generation\n",
    "generate_tts_with_annotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14de17",
   "metadata": {},
   "source": [
    "## Evaluation using SECS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a2249f",
   "metadata": {},
   "source": [
    "#### RAW (BBP - PATAKA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfe99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/lathifgalih-k/miniconda3/envs/yourtts/lib/python3.10/site-packages/resemblyzer/voice_encoder.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cuda in 0.04 seconds.\n",
      "🔍 Evaluating ALS Dataset...\n",
      "\n",
      "=== Evaluating ALS - XTTSv2 ===\n",
      "✅ A017: 0.8566\n",
      "✅ A006: 0.7719\n",
      "✅ A010: 0.8570\n",
      "✅ A008: 0.7615\n",
      "✅ A015: 0.7344\n",
      "✅ A002: 0.7780\n",
      "✅ A014: 0.8088\n",
      "✅ A011: 0.8381\n",
      "✅ A016: 0.7570\n",
      "\n",
      "=== Evaluating ALS - YourTTS ===\n",
      "✅ A014: 0.6696\n",
      "✅ A002: 0.5659\n",
      "✅ A011: 0.7234\n",
      "✅ A016: 0.7141\n",
      "✅ A017: 0.7022\n",
      "✅ A010: 0.7782\n",
      "✅ A006: 0.6900\n",
      "✅ A015: 0.5903\n",
      "✅ A008: 0.5866\n",
      "\n",
      "🔍 Evaluating Stroke Dataset...\n",
      "\n",
      "=== Evaluating Stroke - XTTSv2 ===\n",
      "✅ S011: 0.7287\n",
      "✅ S007: 0.8041\n",
      "✅ S005: 0.7401\n",
      "✅ S013: 0.7719\n",
      "✅ S002: 0.6376\n",
      "✅ S009: 0.7366\n",
      "✅ S003: 0.7010\n",
      "✅ S008: 0.7957\n",
      "✅ S012: 0.8252\n",
      "✅ S001: 0.7477\n",
      "✅ S006: 0.8154\n",
      "\n",
      "=== Evaluating Stroke - YourTTS ===\n",
      "✅ S007: 0.6514\n",
      "✅ S011: 0.5832\n",
      "✅ S013: 0.6525\n",
      "✅ S005: 0.7330\n",
      "✅ S002: 0.5099\n",
      "✅ S009: 0.6252\n",
      "✅ S003: 0.7184\n",
      "✅ S008: 0.7819\n",
      "✅ S012: 0.6933\n",
      "✅ S001: 0.6681\n",
      "✅ S006: 0.6248\n",
      "\n",
      "📊 Summary Results:\n",
      "   Dataset    Model Speaker  Cosine_Similarity  \\\n",
      "0      ALS   XTTSv2    A017           0.856631   \n",
      "1      ALS   XTTSv2    A006           0.771921   \n",
      "2      ALS   XTTSv2    A010           0.857049   \n",
      "3      ALS   XTTSv2    A008           0.761535   \n",
      "4      ALS   XTTSv2    A015           0.734374   \n",
      "5      ALS   XTTSv2    A002           0.777992   \n",
      "6      ALS   XTTSv2    A014           0.808838   \n",
      "7      ALS   XTTSv2    A011           0.838132   \n",
      "8      ALS   XTTSv2    A016           0.756950   \n",
      "9      ALS  YourTTS    A014           0.669622   \n",
      "10     ALS  YourTTS    A002           0.565871   \n",
      "11     ALS  YourTTS    A011           0.723411   \n",
      "12     ALS  YourTTS    A016           0.714089   \n",
      "13     ALS  YourTTS    A017           0.702250   \n",
      "14     ALS  YourTTS    A010           0.778240   \n",
      "15     ALS  YourTTS    A006           0.690045   \n",
      "16     ALS  YourTTS    A015           0.590338   \n",
      "17     ALS  YourTTS    A008           0.586596   \n",
      "18  Stroke   XTTSv2    S011           0.728715   \n",
      "19  Stroke   XTTSv2    S007           0.804124   \n",
      "20  Stroke   XTTSv2    S005           0.740104   \n",
      "21  Stroke   XTTSv2    S013           0.771856   \n",
      "22  Stroke   XTTSv2    S002           0.637566   \n",
      "23  Stroke   XTTSv2    S009           0.736573   \n",
      "24  Stroke   XTTSv2    S003           0.701037   \n",
      "25  Stroke   XTTSv2    S008           0.795698   \n",
      "26  Stroke   XTTSv2    S012           0.825212   \n",
      "27  Stroke   XTTSv2    S001           0.747689   \n",
      "28  Stroke   XTTSv2    S006           0.815416   \n",
      "29  Stroke  YourTTS    S007           0.651365   \n",
      "30  Stroke  YourTTS    S011           0.583200   \n",
      "31  Stroke  YourTTS    S013           0.652515   \n",
      "32  Stroke  YourTTS    S005           0.732965   \n",
      "33  Stroke  YourTTS    S002           0.509938   \n",
      "34  Stroke  YourTTS    S009           0.625190   \n",
      "35  Stroke  YourTTS    S003           0.718405   \n",
      "36  Stroke  YourTTS    S008           0.781921   \n",
      "37  Stroke  YourTTS    S012           0.693287   \n",
      "38  Stroke  YourTTS    S001           0.668067   \n",
      "39  Stroke  YourTTS    S006           0.624781   \n",
      "\n",
      "                                Original_File      Cloned_File  \n",
      "0      input_A017_02_BBP_Normal_segment_0.wav  A017_cloned.wav  \n",
      "1   input_A006_02_BBP_NORMAL_al_segment_0.wav  A006_cloned.wav  \n",
      "2      input_A010_02_BBP_NORMAL_segment_0.wav  A010_cloned.wav  \n",
      "3      input_A008_02_BBP_NORMAL_segment_0.wav  A008_cloned.wav  \n",
      "4      input_A015_02_DDK_PATAKA_segment_0.wav  A015_cloned.wav  \n",
      "5      input_A002_02_BBP_NORMAL_segment_0.wav  A002_cloned.wav  \n",
      "6      input_A014_02_BBP_Normal_segment_0.wav  A014_cloned.wav  \n",
      "7      input_A011_02_BBP_NORMAL_segment_0.wav  A011_cloned.wav  \n",
      "8      input_A016_02_BBP_Normal_segment_0.wav  A016_cloned.wav  \n",
      "9      input_A014_02_BBP_Normal_segment_0.wav  A014_cloned.wav  \n",
      "10     input_A002_02_BBP_NORMAL_segment_0.wav  A002_cloned.wav  \n",
      "11     input_A011_02_BBP_NORMAL_segment_0.wav  A011_cloned.wav  \n",
      "12     input_A016_02_BBP_Normal_segment_0.wav  A016_cloned.wav  \n",
      "13     input_A017_02_BBP_Normal_segment_0.wav  A017_cloned.wav  \n",
      "14     input_A010_02_BBP_NORMAL_segment_0.wav  A010_cloned.wav  \n",
      "15  input_A006_02_BBP_NORMAL_al_segment_0.wav  A006_cloned.wav  \n",
      "16     input_A015_02_DDK_PATAKA_segment_0.wav  A015_cloned.wav  \n",
      "17     input_A008_02_BBP_NORMAL_segment_0.wav  A008_cloned.wav  \n",
      "18     input_S011_02_BBP_NORMAL_segment_0.wav  S011_cloned.wav  \n",
      "19     input_S007_02_BBP_NORMAL_segment_0.wav  S007_cloned.wav  \n",
      "20     input_S005_02_BBP_NORMAL_segment_0.wav  S005_cloned.wav  \n",
      "21     input_S013_02_BBP_NORMAL_segment_0.wav  S013_cloned.wav  \n",
      "22         input_S002_02_DDK_PA_segment_0.wav  S002_cloned.wav  \n",
      "23     input_S009_02_BBP_NORMAL_segment_0.wav  S009_cloned.wav  \n",
      "24     input_S003_02_BBP_NORMAL_segment_0.wav  S003_cloned.wav  \n",
      "25     input_S008_02_BBP_NORMAL_segment_0.wav  S008_cloned.wav  \n",
      "26     input_S012_02_BBP_NORMAL_segment_0.wav  S012_cloned.wav  \n",
      "27         input_S001_02_DDK_PA_segment_0.wav  S001_cloned.wav  \n",
      "28     input_S006_02_BBP_NORMAL_segment_0.wav  S006_cloned.wav  \n",
      "29     input_S007_02_BBP_NORMAL_segment_0.wav  S007_cloned.wav  \n",
      "30     input_S011_02_BBP_NORMAL_segment_0.wav  S011_cloned.wav  \n",
      "31     input_S013_02_BBP_NORMAL_segment_0.wav  S013_cloned.wav  \n",
      "32     input_S005_02_BBP_NORMAL_segment_0.wav  S005_cloned.wav  \n",
      "33         input_S002_02_DDK_PA_segment_0.wav  S002_cloned.wav  \n",
      "34     input_S009_02_BBP_NORMAL_segment_0.wav  S009_cloned.wav  \n",
      "35     input_S003_02_BBP_NORMAL_segment_0.wav  S003_cloned.wav  \n",
      "36     input_S008_02_BBP_NORMAL_segment_0.wav  S008_cloned.wav  \n",
      "37     input_S012_02_BBP_NORMAL_segment_0.wav  S012_cloned.wav  \n",
      "38         input_S001_02_DDK_PA_segment_0.wav  S001_cloned.wav  \n",
      "39     input_S006_02_BBP_NORMAL_segment_0.wav  S006_cloned.wav  \n",
      "\n",
      "📈 Average Cosine Similarity by Dataset and Model:\n",
      "                   mean     std  count\n",
      "Dataset Model                         \n",
      "ALS     XTTSv2   0.7959  0.0458      9\n",
      "        YourTTS  0.6689  0.0725      9\n",
      "Stroke  XTTSv2   0.7549  0.0556     11\n",
      "        YourTTS  0.6583  0.0747     11\n",
      "\n",
      "💾 Results saved to: /home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/speaker_similarity_evaluation.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Evaluation by Cosine Similarity of Speaker Embedding for all generated files\n",
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = VoiceEncoder()\n",
    "\n",
    "def evaluate_speaker_similarity(dataset_type):\n",
    "    \"\"\"\n",
    "    Evaluate speaker similarity for ALS or Stroke dataset\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    if dataset_type == \"ALS\":\n",
    "        input_audio_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/dataset-toronto/ALS/Input_Audios/\"\n",
    "        output_base_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/ALS/\"\n",
    "    else:  # Stroke\n",
    "        input_audio_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/vits/dataset-toronto/Stroke/Input_Audios/\"\n",
    "        output_base_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/Stroke/\"\n",
    "    \n",
    "    # Models to evaluate\n",
    "    models = [\"XTTSv2\", \"YourTTS\"]\n",
    "    \n",
    "    for model in models:\n",
    "        output_dir = os.path.join(output_base_dir, model)\n",
    "        print(f\"\\n=== Evaluating {dataset_type} - {model} ===\")\n",
    "        \n",
    "        # Get all cloned audio files\n",
    "        if os.path.exists(output_dir):\n",
    "            cloned_files = [f for f in os.listdir(output_dir) if f.endswith(\"_cloned.wav\")]\n",
    "            \n",
    "            for cloned_file in cloned_files:\n",
    "                # Extract speaker ID from filename (e.g., \"A002_cloned.wav\" -> \"A002\")\n",
    "                speaker_id = cloned_file.replace(\"_cloned.wav\", \"\")\n",
    "                \n",
    "                # Find corresponding original audio file\n",
    "                speaker_input_dir = os.path.join(input_audio_dir, speaker_id)\n",
    "                \n",
    "                if os.path.exists(speaker_input_dir):\n",
    "                    # Find the appropriate input file based on speaker\n",
    "                    original_file = None\n",
    "                    for audio_file in os.listdir(speaker_input_dir):\n",
    "                        if dataset_type == \"ALS\":\n",
    "                            if speaker_id == \"A015\":\n",
    "                                if \"pataka\" in audio_file.lower() and \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                                    original_file = os.path.join(speaker_input_dir, audio_file)\n",
    "                                    break\n",
    "                            else:\n",
    "                                if \"normal\" in audio_file.lower() and \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                                    original_file = os.path.join(speaker_input_dir, audio_file)\n",
    "                                    break\n",
    "                        else:  # Stroke\n",
    "                            if \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                                original_file = os.path.join(speaker_input_dir, audio_file)\n",
    "                                break\n",
    "                    \n",
    "                    if original_file and os.path.exists(original_file):\n",
    "                        try:\n",
    "                            # Load and preprocess audio files\n",
    "                            original_audio = preprocess_wav(Path(original_file))\n",
    "                            cloned_audio_path = os.path.join(output_dir, cloned_file)\n",
    "                            cloned_audio = preprocess_wav(Path(cloned_audio_path))\n",
    "                            \n",
    "                            # Extract embeddings\n",
    "                            original_embed = encoder.embed_utterance(original_audio)\n",
    "                            cloned_embed = encoder.embed_utterance(cloned_audio)\n",
    "                            \n",
    "                            # Compute cosine similarity\n",
    "                            cos_sim = 1 - cosine(original_embed, cloned_embed)\n",
    "                            \n",
    "                            results.append({\n",
    "                                'Dataset': dataset_type,\n",
    "                                'Model': model,\n",
    "                                'Speaker': speaker_id,\n",
    "                                'Cosine_Similarity': cos_sim,\n",
    "                                'Original_File': os.path.basename(original_file),\n",
    "                                'Cloned_File': cloned_file\n",
    "                            })\n",
    "                            \n",
    "                            print(f\"✅ {speaker_id}: {cos_sim:.4f}\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"❌ Error processing {speaker_id}: {str(e)}\")\n",
    "                    else:\n",
    "                        print(f\"⚠️  Original file not found for {speaker_id}\")\n",
    "                else:\n",
    "                    print(f\"⚠️  Speaker directory not found: {speaker_id}\")\n",
    "        else:\n",
    "            print(f\"⚠️  Output directory not found: {output_dir}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate ALS dataset\n",
    "print(\"🔍 Evaluating ALS Dataset...\")\n",
    "als_results = evaluate_speaker_similarity(\"ALS\")\n",
    "\n",
    "# Evaluate Stroke dataset  \n",
    "print(\"\\n🔍 Evaluating Stroke Dataset...\")\n",
    "stroke_results = evaluate_speaker_similarity(\"Stroke\")\n",
    "\n",
    "# Combine all results\n",
    "all_results = als_results + stroke_results\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df_results = pd.DataFrame(all_results)\n",
    "print(\"\\n📊 Summary Results:\")\n",
    "print(df_results)\n",
    "\n",
    "# Calculate average similarity by dataset and model\n",
    "if not df_results.empty:\n",
    "    print(\"\\n📈 Average Cosine Similarity by Dataset and Model:\")\n",
    "    summary = df_results.groupby(['Dataset', 'Model'])['Cosine_Similarity'].agg(['mean', 'std', 'count']).round(4)\n",
    "    print(summary)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    output_csv = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/RAW/speaker_similarity_evaluation.csv\"\n",
    "    df_results.to_csv(output_csv, index=False)\n",
    "    print(f\"\\n💾 Results saved to: {output_csv}\")\n",
    "else:\n",
    "    print(\"⚠️  No results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0512a",
   "metadata": {},
   "source": [
    "#### VCTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa45aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/lathifgalih-k/miniconda3/envs/yourtts/lib/python3.10/site-packages/resemblyzer/voice_encoder.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cuda in 0.45 seconds.\n",
      "🔍 Evaluating ALS Dataset...\n",
      "\n",
      "=== Evaluating ALS - XTTSv2 ===\n",
      "✅ A010: 0.7874\n",
      "✅ A006: 0.7039\n",
      "✅ A017: 0.6625\n",
      "✅ A015: 0.7649\n",
      "✅ A008: 0.7078\n",
      "✅ A014: 0.7256\n",
      "✅ A002: 0.7232\n",
      "✅ A016: 0.7125\n",
      "✅ A011: 0.7315\n",
      "\n",
      "=== Evaluating ALS - YourTTS ===\n",
      "✅ A008: 0.5959\n",
      "✅ A015: 0.6945\n",
      "✅ A017: 0.6269\n",
      "✅ A006: 0.6290\n",
      "✅ A010: 0.6669\n",
      "✅ A011: 0.7286\n",
      "✅ A016: 0.7109\n",
      "✅ A002: 0.6123\n",
      "✅ A014: 0.6720\n",
      "\n",
      "🔍 Evaluating Stroke Dataset...\n",
      "\n",
      "=== Evaluating Stroke - XTTSv2 ===\n",
      "✅ S007: 0.7961\n",
      "✅ S011: 0.7520\n",
      "✅ S009: 0.7314\n",
      "✅ S002: 0.6883\n",
      "✅ S013: 0.6702\n",
      "✅ S005: 0.6286\n",
      "✅ S012: 0.7610\n",
      "✅ S008: 0.8108\n",
      "✅ S003: 0.6897\n",
      "✅ S006: 0.7438\n",
      "✅ S001: 0.5701\n",
      "\n",
      "=== Evaluating Stroke - YourTTS ===\n",
      "✅ S001: 0.5970\n",
      "✅ S006: 0.5595\n",
      "✅ S003: 0.6358\n",
      "✅ S008: 0.6954\n",
      "✅ S012: 0.6687\n",
      "✅ S005: 0.6947\n",
      "✅ S013: 0.5819\n",
      "✅ S002: 0.6278\n",
      "✅ S009: 0.5549\n",
      "✅ S011: 0.6282\n",
      "✅ S007: 0.6720\n",
      "\n",
      "📊 Summary Results:\n",
      "   Dataset    Model Speaker  Cosine_Similarity  \\\n",
      "0      ALS   XTTSv2    A010           0.787390   \n",
      "1      ALS   XTTSv2    A006           0.703922   \n",
      "2      ALS   XTTSv2    A017           0.662469   \n",
      "3      ALS   XTTSv2    A015           0.764913   \n",
      "4      ALS   XTTSv2    A008           0.707838   \n",
      "5      ALS   XTTSv2    A014           0.725627   \n",
      "6      ALS   XTTSv2    A002           0.723191   \n",
      "7      ALS   XTTSv2    A016           0.712493   \n",
      "8      ALS   XTTSv2    A011           0.731501   \n",
      "9      ALS  YourTTS    A008           0.595903   \n",
      "10     ALS  YourTTS    A015           0.694485   \n",
      "11     ALS  YourTTS    A017           0.626901   \n",
      "12     ALS  YourTTS    A006           0.629026   \n",
      "13     ALS  YourTTS    A010           0.666928   \n",
      "14     ALS  YourTTS    A011           0.728611   \n",
      "15     ALS  YourTTS    A016           0.710850   \n",
      "16     ALS  YourTTS    A002           0.612300   \n",
      "17     ALS  YourTTS    A014           0.671977   \n",
      "18  Stroke   XTTSv2    S007           0.796122   \n",
      "19  Stroke   XTTSv2    S011           0.752022   \n",
      "20  Stroke   XTTSv2    S009           0.731385   \n",
      "21  Stroke   XTTSv2    S002           0.688300   \n",
      "22  Stroke   XTTSv2    S013           0.670200   \n",
      "23  Stroke   XTTSv2    S005           0.628597   \n",
      "24  Stroke   XTTSv2    S012           0.760963   \n",
      "25  Stroke   XTTSv2    S008           0.810829   \n",
      "26  Stroke   XTTSv2    S003           0.689665   \n",
      "27  Stroke   XTTSv2    S006           0.743752   \n",
      "28  Stroke   XTTSv2    S001           0.570132   \n",
      "29  Stroke  YourTTS    S001           0.597016   \n",
      "30  Stroke  YourTTS    S006           0.559484   \n",
      "31  Stroke  YourTTS    S003           0.635794   \n",
      "32  Stroke  YourTTS    S008           0.695441   \n",
      "33  Stroke  YourTTS    S012           0.668683   \n",
      "34  Stroke  YourTTS    S005           0.694733   \n",
      "35  Stroke  YourTTS    S013           0.581871   \n",
      "36  Stroke  YourTTS    S002           0.627826   \n",
      "37  Stroke  YourTTS    S009           0.554872   \n",
      "38  Stroke  YourTTS    S011           0.628229   \n",
      "39  Stroke  YourTTS    S007           0.671966   \n",
      "\n",
      "                                Original_File      Cloned_File  \n",
      "0      input_A010_02_BBP_NORMAL_segment_0.wav  A010_cloned.wav  \n",
      "1   input_A006_02_BBP_NORMAL_al_segment_0.wav  A006_cloned.wav  \n",
      "2      input_A017_02_BBP_Normal_segment_0.wav  A017_cloned.wav  \n",
      "3      input_A015_02_DDK_PATAKA_segment_0.wav  A015_cloned.wav  \n",
      "4      input_A008_02_BBP_NORMAL_segment_0.wav  A008_cloned.wav  \n",
      "5      input_A014_02_BBP_Normal_segment_0.wav  A014_cloned.wav  \n",
      "6      input_A002_02_BBP_NORMAL_segment_0.wav  A002_cloned.wav  \n",
      "7      input_A016_02_BBP_Normal_segment_0.wav  A016_cloned.wav  \n",
      "8      input_A011_02_BBP_NORMAL_segment_0.wav  A011_cloned.wav  \n",
      "9      input_A008_02_BBP_NORMAL_segment_0.wav  A008_cloned.wav  \n",
      "10     input_A015_02_DDK_PATAKA_segment_0.wav  A015_cloned.wav  \n",
      "11     input_A017_02_BBP_Normal_segment_0.wav  A017_cloned.wav  \n",
      "12  input_A006_02_BBP_NORMAL_al_segment_0.wav  A006_cloned.wav  \n",
      "13     input_A010_02_BBP_NORMAL_segment_0.wav  A010_cloned.wav  \n",
      "14     input_A011_02_BBP_NORMAL_segment_0.wav  A011_cloned.wav  \n",
      "15     input_A016_02_BBP_Normal_segment_0.wav  A016_cloned.wav  \n",
      "16     input_A002_02_BBP_NORMAL_segment_0.wav  A002_cloned.wav  \n",
      "17     input_A014_02_BBP_Normal_segment_0.wav  A014_cloned.wav  \n",
      "18     input_S007_02_BBP_NORMAL_segment_0.wav  S007_cloned.wav  \n",
      "19     input_S011_02_BBP_NORMAL_segment_0.wav  S011_cloned.wav  \n",
      "20     input_S009_02_BBP_NORMAL_segment_0.wav  S009_cloned.wav  \n",
      "21         input_S002_02_DDK_PA_segment_0.wav  S002_cloned.wav  \n",
      "22     input_S013_02_BBP_NORMAL_segment_0.wav  S013_cloned.wav  \n",
      "23     input_S005_02_BBP_NORMAL_segment_0.wav  S005_cloned.wav  \n",
      "24     input_S012_02_BBP_NORMAL_segment_0.wav  S012_cloned.wav  \n",
      "25     input_S008_02_BBP_NORMAL_segment_0.wav  S008_cloned.wav  \n",
      "26     input_S003_02_BBP_NORMAL_segment_0.wav  S003_cloned.wav  \n",
      "27     input_S006_02_BBP_NORMAL_segment_0.wav  S006_cloned.wav  \n",
      "28         input_S001_02_DDK_PA_segment_0.wav  S001_cloned.wav  \n",
      "29         input_S001_02_DDK_PA_segment_0.wav  S001_cloned.wav  \n",
      "30     input_S006_02_BBP_NORMAL_segment_0.wav  S006_cloned.wav  \n",
      "31     input_S003_02_BBP_NORMAL_segment_0.wav  S003_cloned.wav  \n",
      "32     input_S008_02_BBP_NORMAL_segment_0.wav  S008_cloned.wav  \n",
      "33     input_S012_02_BBP_NORMAL_segment_0.wav  S012_cloned.wav  \n",
      "34     input_S005_02_BBP_NORMAL_segment_0.wav  S005_cloned.wav  \n",
      "35     input_S013_02_BBP_NORMAL_segment_0.wav  S013_cloned.wav  \n",
      "36         input_S002_02_DDK_PA_segment_0.wav  S002_cloned.wav  \n",
      "37     input_S009_02_BBP_NORMAL_segment_0.wav  S009_cloned.wav  \n",
      "38     input_S011_02_BBP_NORMAL_segment_0.wav  S011_cloned.wav  \n",
      "39     input_S007_02_BBP_NORMAL_segment_0.wav  S007_cloned.wav  \n",
      "\n",
      "📈 Average Cosine Similarity by Dataset and Model:\n",
      "                   mean     std  count\n",
      "Dataset Model                         \n",
      "ALS     XTTSv2   0.7244  0.0359      9\n",
      "        YourTTS  0.6597  0.0462      9\n",
      "Stroke  XTTSv2   0.7129  0.0722     11\n",
      "        YourTTS  0.6287  0.0508     11\n",
      "\n",
      "💾 Results saved to: speaker_similarity_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluation by Cosine Similarity of Speaker Embedding for all generated files\n",
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = VoiceEncoder()\n",
    "\n",
    "def evaluate_speaker_similarity(dataset_type):\n",
    "    \"\"\"\n",
    "    Evaluate speaker similarity for ALS or Stroke dataset\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    if dataset_type == \"ALS\":\n",
    "        input_audio_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/dataset-toronto/ALS/Input_Audios/\"\n",
    "        output_base_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/VCTK/ALS/\"\n",
    "    else:  # Stroke\n",
    "        input_audio_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/vits/dataset-toronto/Stroke/Input_Audios/\"\n",
    "        output_base_dir = \"/home/is/lathifgalih-k/research_naist/multimodal-clone/clone-voice-coqui/output_audio/VCTK/Stroke\"\n",
    "    \n",
    "    # Models to evaluate\n",
    "    models = [\"XTTSv2\", \"YourTTS\"]\n",
    "    \n",
    "    for model in models:\n",
    "        output_dir = os.path.join(output_base_dir, model)\n",
    "        print(f\"\\n=== Evaluating {dataset_type} - {model} ===\")\n",
    "        \n",
    "        # Get all cloned audio files\n",
    "        if os.path.exists(output_dir):\n",
    "            cloned_files = [f for f in os.listdir(output_dir) if f.endswith(\"_cloned.wav\")]\n",
    "            \n",
    "            for cloned_file in cloned_files:\n",
    "                # Extract speaker ID from filename (e.g., \"A002_cloned.wav\" -> \"A002\")\n",
    "                speaker_id = cloned_file.replace(\"_cloned.wav\", \"\")\n",
    "                \n",
    "                # Find corresponding original audio file\n",
    "                speaker_input_dir = os.path.join(input_audio_dir, speaker_id)\n",
    "                \n",
    "                if os.path.exists(speaker_input_dir):\n",
    "                    # Find the appropriate input file based on speaker\n",
    "                    original_file = None\n",
    "                    for audio_file in os.listdir(speaker_input_dir):\n",
    "                        if dataset_type == \"ALS\":\n",
    "                            if speaker_id == \"A015\":\n",
    "                                if \"pataka\" in audio_file.lower() and \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                                    original_file = os.path.join(speaker_input_dir, audio_file)\n",
    "                                    break\n",
    "                            else:\n",
    "                                if \"normal\" in audio_file.lower() and \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                                    original_file = os.path.join(speaker_input_dir, audio_file)\n",
    "                                    break\n",
    "                        else:  # Stroke\n",
    "                            if \"segment_0\" in audio_file.lower() and audio_file.endswith(\".wav\"):\n",
    "                                original_file = os.path.join(speaker_input_dir, audio_file)\n",
    "                                break\n",
    "                    \n",
    "                    if original_file and os.path.exists(original_file):\n",
    "                        try:\n",
    "                            # Load and preprocess audio files\n",
    "                            original_audio = preprocess_wav(Path(original_file))\n",
    "                            cloned_audio_path = os.path.join(output_dir, cloned_file)\n",
    "                            cloned_audio = preprocess_wav(Path(cloned_audio_path))\n",
    "                            \n",
    "                            # Extract embeddings\n",
    "                            original_embed = encoder.embed_utterance(original_audio)\n",
    "                            cloned_embed = encoder.embed_utterance(cloned_audio)\n",
    "                            \n",
    "                            # Compute cosine similarity\n",
    "                            cos_sim = 1 - cosine(original_embed, cloned_embed)\n",
    "                            \n",
    "                            results.append({\n",
    "                                'Dataset': dataset_type,\n",
    "                                'Model': model,\n",
    "                                'Speaker': speaker_id,\n",
    "                                'Cosine_Similarity': cos_sim,\n",
    "                                'Original_File': os.path.basename(original_file),\n",
    "                                'Cloned_File': cloned_file\n",
    "                            })\n",
    "                            \n",
    "                            print(f\"✅ {speaker_id}: {cos_sim:.4f}\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"❌ Error processing {speaker_id}: {str(e)}\")\n",
    "                    else:\n",
    "                        print(f\"⚠️  Original file not found for {speaker_id}\")\n",
    "                else:\n",
    "                    print(f\"⚠️  Speaker directory not found: {speaker_id}\")\n",
    "        else:\n",
    "            print(f\"⚠️  Output directory not found: {output_dir}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate ALS dataset\n",
    "print(\"🔍 Evaluating ALS Dataset...\")\n",
    "als_results = evaluate_speaker_similarity(\"ALS\")\n",
    "\n",
    "# Evaluate Stroke dataset  \n",
    "print(\"\\n🔍 Evaluating Stroke Dataset...\")\n",
    "stroke_results = evaluate_speaker_similarity(\"Stroke\")\n",
    "\n",
    "# Combine all results\n",
    "all_results = als_results + stroke_results\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df_results = pd.DataFrame(all_results)\n",
    "print(\"\\n📊 Summary Results:\")\n",
    "print(df_results)\n",
    "\n",
    "# Calculate average similarity by dataset and model\n",
    "if not df_results.empty:\n",
    "    print(\"\\n📈 Average Cosine Similarity by Dataset and Model:\")\n",
    "    summary = df_results.groupby(['Dataset', 'Model'])['Cosine_Similarity'].agg(['mean', 'std', 'count']).round(4)\n",
    "    print(summary)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    output_csv = \"speaker_similarity_evaluation.csv\"\n",
    "    df_results.to_csv(output_csv, index=False)\n",
    "    print(f\"\\n💾 Results saved to: {output_csv}\")\n",
    "else:\n",
    "    print(\"⚠️  No results to display\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yourtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
